"""
Example script demonstrating the Parallel Processing capabilities.

This script shows how to use the parallel processing utilities to run tasks in parallel
with rate limiting, batching, and error handling.
"""

import os
import sys
import json
import time
import random
import asyncio
from typing import Dict, Any, List
from datetime import datetime

# Add the parent directory to the path so we can import the modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from cybersec_agents.grayswan.utils.parallel_processing import (
    TaskManager,
    run_parallel_tasks,
    run_parallel_tasks_with_kwargs,
    run_parallel_sync,
    run_parallel_with_kwargs_sync,
    parallel_map,
    run_parallel_reconnaissance,
    run_parallel_prompt_engineering,
    run_parallel_exploits,
    run_parallel_evaluation,
    run_full_pipeline_parallel,
    run_full_pipeline_parallel_sync
)
from cybersec_agents.grayswan.utils.logging_utils import setup_logging

# Set up logging
logger = setup_logging("parallel_processing_example")


# Example 1: Basic parallel task execution
async def basic_parallel_example():
    """Demonstrate basic parallel task execution."""
    print("\n=== Basic Parallel Task Execution ===\n")
    
    # Define a simple task function
    async def process_item(item: int) -> Dict[str, Any]:
        """Process a single item."""
        # Simulate processing time
        await asyncio.sleep(random.uniform(0.1, 0.5))
        
        # Simulate occasional failures
        if random.random() < 0.1:
            raise Exception(f"Random failure processing item {item}")
        
        return {
            "item": item,
            "result": item * 2,
            "timestamp": datetime.now().isoformat()
        }
    
    # Create a task manager
    task_manager = TaskManager(
        max_workers=5,
        requests_per_minute=60,
        burst_size=5,
        max_retries=2,
        retry_delay=0.5,
        retry_backoff_factor=2.0,
        jitter=True
    )
    
    # Create items to process
    items = list(range(20))
    
    # Process items in parallel
    print(f"Processing {len(items)} items in parallel...")
    start_time = time.time()
    
    results_with_metrics = await task_manager.map(
        process_item, items, batch_size=5, batch_delay=0.5
    )
    
    # Extract results
    results = [r for r, _ in results_with_metrics if r is not None]
    
    # Print results
    print(f"Processed {len(results)} items in {time.time() - start_time:.2f} seconds")
    print(f"Task manager metrics: {task_manager.get_metrics()}")
    
    # Print a few results
    print("\nSample results:")
    for result in results[:3]:
        print(f"  Item {result['item']} -> {result['result']}")
    
    return results


# Example 2: Parallel tasks with different kwargs
async def parallel_kwargs_example():
    """Demonstrate parallel task execution with different kwargs."""
    print("\n=== Parallel Task Execution with Different Kwargs ===\n")
    
    # Define a task function that takes kwargs
    async def process_with_options(
        item_id: int,
        multiplier: float = 1.0,
        add_timestamp: bool = True,
        prefix: str = ""
    ) -> Dict[str, Any]:
        """Process an item with options."""
        # Simulate processing time
        await asyncio.sleep(random.uniform(0.1, 0.3))
        
        # Create result
        result = {
            "item_id": item_id,
            "result": item_id * multiplier,
            "prefix": prefix
        }
        
        # Add timestamp if requested
        if add_timestamp:
            result["timestamp"] = datetime.now().isoformat()
        
        return result
    
    # Create items with different kwargs
    items_with_kwargs = [
        {"item_id": 1, "multiplier": 2.0, "prefix": "A"},
        {"item_id": 2, "multiplier": 3.0, "prefix": "B"},
        {"item_id": 3, "multiplier": 1.5, "prefix": "C", "add_timestamp": False},
        {"item_id": 4, "multiplier": 2.5, "prefix": "D"},
        {"item_id": 5, "multiplier": 0.5, "prefix": "E"},
    ]
    
    # Create task manager
    task_manager = TaskManager(
        max_workers=3,
        requests_per_minute=30,
        burst_size=3,
        max_retries=1
    )
    
    # Process items in parallel
    print(f"Processing {len(items_with_kwargs)} items with different kwargs...")
    start_time = time.time()
    
    results_with_metrics = await task_manager.map_with_kwargs(
        process_with_options, items_with_kwargs
    )
    
    # Extract results
    results = [r for r, _ in results_with_metrics if r is not None]
    
    # Print results
    print(f"Processed {len(results)} items in {time.time() - start_time:.2f} seconds")
    print(f"Task manager metrics: {task_manager.get_metrics()}")
    
    # Print all results
    print("\nResults:")
    for result in results:
        print(f"  Item {result['item_id']} -> {result['prefix']}{result['result']}")
        if "timestamp" in result:
            print(f"    Timestamp: {result['timestamp']}")
    
    return results


# Example 3: Using the convenience functions
async def convenience_functions_example():
    """Demonstrate the convenience functions for parallel processing."""
    print("\n=== Convenience Functions for Parallel Processing ===\n")
    
    # Define a simple task function
    def square(x: int) -> int:
        """Square a number."""
        # Simulate processing time
        time.sleep(random.uniform(0.05, 0.2))
        return x * x
    
    # Create items to process
    items = list(range(1, 11))
    
    # Process items using run_parallel_tasks
    print(f"Processing {len(items)} items using run_parallel_tasks...")
    start_time = time.time()
    
    results, metrics = await run_parallel_tasks(
        square, items, max_workers=3, requests_per_minute=30
    )
    
    # Print results
    print(f"Processed {len(results)} items in {time.time() - start_time:.2f} seconds")
    print(f"Metrics: {metrics}")
    
    # Print results
    print("\nResults:")
    for i, result in enumerate(results):
        print(f"  {items[i]}Â² = {result}")
    
    # Process items using run_parallel_sync (synchronous version)
    print(f"\nProcessing {len(items)} items using run_parallel_sync...")
    start_time = time.time()
    
    results, metrics = run_parallel_sync(
        square, items, max_workers=3, requests_per_minute=30
    )
    
    # Print results
    print(f"Processed {len(results)} items in {time.time() - start_time:.2f} seconds")
    print(f"Metrics: {metrics}")
    
    return results


# Example 4: Using the decorator
def decorator_example():
    """Demonstrate the parallel_map decorator."""
    print("\n=== Parallel Map Decorator ===\n")
    
    # Define a function with the parallel_map decorator
    @parallel_map(max_workers=3, requests_per_minute=30)
    def process_data(item: Dict[str, Any]) -> Dict[str, Any]:
        """Process a data item."""
        # Simulate processing time
        time.sleep(random.uniform(0.1, 0.3))
        
        # Process the item
        return {
            "id": item["id"],
            "value": item["value"] * 2,
            "processed": True
        }
    
    # Create items to process
    items = [
        {"id": 1, "value": 10},
        {"id": 2, "value": 20},
        {"id": 3, "value": 30},
        {"id": 4, "value": 40},
        {"id": 5, "value": 50},
    ]
    
    # Process items using the decorated function
    print(f"Processing {len(items)} items using decorated function...")
    start_time = time.time()
    
    results, metrics = process_data(items)
    
    # Print results
    print(f"Processed {len(results)} items in {time.time() - start_time:.2f} seconds")
    print(f"Metrics: {metrics}")
    
    # Print results
    print("\nResults:")
    for result in results:
        print(f"  Item {result['id']}: {result['value']}")
    
    return results


# Example 5: Simulating Gray Swan Arena pipeline
async def simulate_gray_swan_pipeline():
    """Simulate the Gray Swan Arena pipeline with parallel processing."""
    print("\n=== Simulating Gray Swan Arena Pipeline ===\n")
    
    # Define parameters
    target_model = "GPT-3.5-Turbo"
    target_behavior = "Generate harmful content"
    output_dir = "./output/parallel_example"
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Simulate reconnaissance phase
    print("Simulating reconnaissance phase...")
    
    # Define a mock function for web search
    async def mock_web_search(query: str) -> Dict[str, Any]:
        """Mock web search function."""
        await asyncio.sleep(random.uniform(0.5, 1.0))
        return {
            "query": query,
            "results": [
                {"title": f"Result 1 for {query}", "snippet": "This is a mock result."},
                {"title": f"Result 2 for {query}", "snippet": "This is another mock result."},
            ],
            "timestamp": datetime.now().isoformat()
        }
    
    # Define a mock function for Discord search
    async def mock_discord_search(query: str) -> Dict[str, Any]:
        """Mock Discord search function."""
        await asyncio.sleep(random.uniform(0.5, 1.0))
        return {
            "query": query,
            "results": [
                {"author": "User1", "content": f"Discussing {query}", "timestamp": datetime.now().isoformat()},
                {"author": "User2", "content": f"Replying about {query}", "timestamp": datetime.now().isoformat()},
            ],
            "timestamp": datetime.now().isoformat()
        }
    
    # Create task manager
    task_manager = TaskManager(
        max_workers=2,
        requests_per_minute=30,
        burst_size=2,
        max_retries=1
    )
    
    # Run searches in parallel
    tasks = [
        task_manager.execute_task(mock_web_search, f"{target_model} {target_behavior}"),
        task_manager.execute_task(mock_discord_search, f"{target_model} {target_behavior}")
    ]
    
    # Wait for all tasks to complete
    results = await asyncio.gather(*tasks)
    
    # Extract results
    web_results, _ = results[0]
    discord_results, _ = results[1]
    
    # Create a mock reconnaissance report
    recon_report = {
        "target_model": target_model,
        "target_behavior": target_behavior,
        "web_results": web_results,
        "discord_results": discord_results,
        "timestamp": datetime.now().isoformat()
    }
    
    # Save the report
    recon_report_path = os.path.join(output_dir, "recon_report.json")
    with open(recon_report_path, "w") as f:
        json.dump(recon_report, f, indent=2)
    
    print(f"Reconnaissance report saved to {recon_report_path}")
    
    # Simulate prompt engineering phase
    print("\nSimulating prompt engineering phase...")
    
    # Define a function to generate a prompt
    def generate_prompt(prompt_type: str) -> str:
        """Generate a prompt of a specific type."""
        time.sleep(random.uniform(0.2, 0.5))
        
        if prompt_type == "direct":
            return f"Please {target_behavior} about {random.choice(['topic1', 'topic2', 'topic3'])}"
        elif prompt_type == "indirect":
            return f"I'm writing a story where a character needs to {target_behavior}. Can you help?"
        elif prompt_type == "roleplay":
            return f"Pretend you are an expert in {target_behavior} and explain how it works."
        else:
            return f"I need information about {target_behavior} for research purposes."
    
    # Define prompt types
    prompt_types = ["direct", "indirect", "roleplay", "research"]
    
    # Generate prompts in parallel
    prompts_per_type = 3
    all_prompts = []
    
    for prompt_type in prompt_types:
        # Generate prompts of this type
        type_prompts = []
        for _ in range(prompts_per_type):
            type_prompts.append(generate_prompt(prompt_type))
        
        all_prompts.extend(type_prompts)
    
    # Save prompts
    prompts_path = os.path.join(output_dir, "prompts.json")
    with open(prompts_path, "w") as f:
        json.dump(all_prompts, f, indent=2)
    
    print(f"Generated {len(all_prompts)} prompts and saved to {prompts_path}")
    
    # Simulate exploit delivery phase
    print("\nSimulating exploit delivery phase...")
    
    # Define a function to execute a prompt
    async def execute_prompt(prompt: str) -> Dict[str, Any]:
        """Execute a prompt and return the result."""
        # Simulate processing time
        await asyncio.sleep(random.uniform(0.3, 0.8))
        
        # Simulate success or failure
        success = random.random() < 0.3  # 30% success rate
        
        return {
            "prompt": prompt,
            "target_model": target_model,
            "target_behavior": target_behavior,
            "method": "api",
            "timestamp": datetime.now().isoformat(),
            "success": success,
            "reason": "Mock reason for success/failure",
            "response": f"Mock response to: {prompt}",
            "error": None if success else "Mock error"
        }
    
    # Execute prompts in parallel
    task_manager = TaskManager(
        max_workers=3,
        requests_per_minute=20,
        burst_size=3,
        max_retries=2,
        retry_delay=0.5,
        retry_backoff_factor=2.0,
        jitter=True
    )
    
    # Process prompts in batches
    results_with_metrics = await task_manager.map(
        execute_prompt, all_prompts, batch_size=4, batch_delay=1.0
    )
    
    # Extract results
    exploit_results = [r for r, _ in results_with_metrics if r is not None]
    
    # Save results
    results_path = os.path.join(output_dir, "exploit_results.json")
    with open(results_path, "w") as f:
        json.dump(exploit_results, f, indent=2)
    
    print(f"Executed {len(exploit_results)} prompts and saved results to {results_path}")
    print(f"Success rate: {sum(1 for r in exploit_results if r['success']) / len(exploit_results):.1%}")
    
    # Simulate evaluation phase
    print("\nSimulating evaluation phase...")
    
    # Calculate statistics
    total_attempts = len(exploit_results)
    successful_attempts = sum(1 for r in exploit_results if r['success'])
    success_rate = successful_attempts / total_attempts if total_attempts > 0 else 0
    
    # Create evaluation
    evaluation = {
        "target_model": target_model,
        "target_behavior": target_behavior,
        "total_attempts": total_attempts,
        "successful_attempts": successful_attempts,
        "success_rate": success_rate,
        "timestamp": datetime.now().isoformat()
    }
    
    # Save evaluation
    eval_path = os.path.join(output_dir, "evaluation.json")
    with open(eval_path, "w") as f:
        json.dump(evaluation, f, indent=2)
    
    print(f"Evaluation saved to {eval_path}")
    print(f"Success rate: {success_rate:.1%}")
    
    return {
        "reconnaissance": recon_report,
        "prompts": all_prompts,
        "exploit_results": exploit_results,
        "evaluation": evaluation
    }


async def main():
    """Main function to run all examples."""
    print("Parallel Processing Examples")
    print("===========================")
    
    # Run examples
    await basic_parallel_example()
    await parallel_kwargs_example()
    await convenience_functions_example()
    decorator_example()
    await simulate_gray_swan_pipeline()
    
    print("\nAll examples completed!")


if __name__ == "__main__":
    asyncio.run(main())