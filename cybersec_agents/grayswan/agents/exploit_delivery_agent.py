"""
Exploit Delivery Agent for Gray Swan Arena.

This agent is responsible for delivering attack prompts to target AI models
and recording the responses for evaluation.
"""

import json
import os
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from functools import wraps
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
import logging

from camel.agents import ChatAgent
from camel.types import ModelType, ModelPlatformType
from ..utils.model_factory import get_chat_agent
from ..utils.logging_utils import setup_logging
from ..utils.model_config_manager import ModelConfigManager
from ..utils.model_utils import _get_model_type, _get_model_platform

# Set up logging
logger = logging.getLogger(__name__)
setup_logging("exploit_delivery_agent")


# Define a local version of with_exponential_backoff to avoid circular imports
def with_exponential_backoff(max_retries: int = 3, initial_delay: float = 1.0):
    """Decorator for retrying functions with exponential backoff.
    
    Args:
        max_retries: Maximum number of retries
        initial_delay: Initial delay in seconds
        
    Returns:
        Decorated function
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    # Check if we should retry
                    if attempt == max_retries - 1:
                        break
                    
                    # Add jitter to delay
                    actual_delay = delay * (0.75 + random.random() * 0.5)
                    
                    logger.warning(
                        f"Attempt {attempt + 1}/{max_retries} failed: {str(e)}. "
                        f"Retrying in {actual_delay:.2f} seconds..."
                    )
                    
                    # Wait before retrying
                    time.sleep(actual_delay)
                    
                    # Increase delay for next retry
                    delay *= 2
            
            # If we get here, all retries failed
            if last_exception:
                raise last_exception
            
            raise Exception(f"All {max_retries} attempts failed")
        
        return wrapper
    
    return decorator


class ExploitDeliveryAgent:
    """Agent for delivering exploits to a target model."""

    def __init__(
        self,
        model_type: Optional[ModelType] = None,
        model_platform: Optional[ModelPlatformType] = None,
        model_name: Optional[str] = None,
        backup_model_type: Optional[ModelType] = None,
        backup_model_platform: Optional[ModelPlatformType] = None,
        reasoning_model_type: Optional[ModelType] = None,
        reasoning_model_platform: Optional[ModelPlatformType] = None,
        reasoning_model: Optional[str] = None,
        api_key: Optional[str] = None,
        output_dir: str = "./exploits",
        **kwargs: Any,
    ) -> None:
        """Initialize the ExploitDeliveryAgent.

        Args:
            model_type: Type of model to use (e.g. GPT_4, CLAUDE_3_SONNET)
            model_platform: Platform to use (e.g. OPENAI, ANTHROPIC)
            model_name: Optional name of the model to use (for backwards compatibility)
            backup_model_type: Type of backup model to use if primary fails
            backup_model_platform: Platform of backup model
            reasoning_model_type: Type of model to use for reasoning tasks
            reasoning_model_platform: Platform of reasoning model
            reasoning_model: Optional name of reasoning model (for backwards compatibility)
            api_key: Optional API key for the model
            output_dir: Directory to save results to
            **kwargs: Additional arguments to pass to the model
        """
        super().__init__()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Determine model parameters
        self.model_name = model_name
        self.model_type = model_type
        self.model_platform = model_platform
        
        # If model_type/platform not provided but model_name is, derive them from model_name
        if (self.model_type is None or self.model_platform is None) and self.model_name:
            self.model_type = self.model_type or _get_model_type(self.model_name)
            self.model_platform = self.model_platform or _get_model_platform(self.model_name)
            
        # Set target model properties with defaults if needed
        self.target_model_type = self.model_type or ModelType.GPT_4
        self.target_model_platform = self.model_platform or ModelPlatformType.OPENAI
        
        # Setup backup model configuration
        self.backup_model_type = backup_model_type
        self.backup_model_platform = backup_model_platform
        
        # Determine reasoning model parameters
        self.reasoning_model = reasoning_model
        self.reasoning_model_type = reasoning_model_type or self.target_model_type
        self.reasoning_model_platform = reasoning_model_platform or self.target_model_platform
        
        # If reasoning model name is provided but not type/platform, derive them
        if (self.reasoning_model_type is None or self.reasoning_model_platform is None) and self.reasoning_model:
            self.reasoning_model_type = self.reasoning_model_type or _get_model_type(self.reasoning_model)
            self.reasoning_model_platform = self.reasoning_model_platform or _get_model_platform(self.reasoning_model)
        
        self.api_key = api_key

        # Helper method to convert model type to model name (for backward compatibility)
        def _get_model_name_from_type(model_type: ModelType) -> str:
            """Convert ModelType to model name string.
            
            Args:
                model_type: ModelType enum value
                
            Returns:
                str: Model name as a string
            """
            model_type_to_name = {
                ModelType.GPT_4: "gpt-4",
                ModelType.GPT_4_TURBO: "gpt-4-turbo",
                ModelType.GPT_3_5_TURBO: "gpt-3.5-turbo",
                ModelType.CLAUDE_3_SONNET: "claude-3-sonnet",
                ModelType.CLAUDE_3_OPUS: "claude-3-opus",
                # Add other model types as they become available
            }
            return model_type_to_name.get(model_type, "gpt-4")  # Default to gpt-4 if unknown
        
        # Initialize model-specific agents
        if self.target_model_platform == ModelPlatformType.ANTHROPIC:
            self.claude_chat_agent = get_chat_agent(
                model_name=_get_model_name_from_type(self.target_model_type),
                model_type=self.target_model_type,
                model_platform=ModelPlatformType.ANTHROPIC,
                api_key=self.api_key,
                **kwargs,
            )
        else:
            # Default to OpenAI models
            self.gpt_chat_agent = get_chat_agent(
                model_name=_get_model_name_from_type(self.target_model_type),
                model_type=self.target_model_type,
                model_platform=ModelPlatformType.OPENAI,
                api_key=self.api_key,
                **kwargs,
            )

        # Initialize the ModelConfigManager
        self.model_manager = ModelConfigManager()
        self.model_config = self.model_manager.get_model_params(
            self.target_model_type,
            self.target_model_platform
        ) or {}

        # Improved initialization log
        logger.info("ExploitDeliveryAgent initialized with model type: %s, platform: %s",
                   self.target_model_type,
                   self.target_model_platform)
        
        # If backup model is configured, log it
        if self.backup_model_type:
            logger.info("Backup model configured: %s/%s",
                       self.backup_model_type,
                       self.backup_model_platform)
            
        # If reasoning model differs from main model, log it
        if self.reasoning_model_type != self.target_model_type or self.reasoning_model_platform != self.target_model_platform:
            logger.info("Reasoning model configured: %s/%s",
                       self.reasoning_model_type,
                       self.reasoning_model_platform)
        
        # Log model configuration
        logger.info("Using model configuration: %s", self.model_config)

    @with_exponential_backoff(max_retries=3)
    def execute_prompt_gemini(self, prompt: str) -> str:
        """Execute a prompt using the Gemini model.

        Args:
            prompt: The prompt to execute

        Returns:
            The model's response
        """
        response = self.gpt_chat_agent.step(prompt)
        return response.msg.content

    @with_exponential_backoff(max_retries=3)
    def execute_prompt_perplexity(self, prompt: str) -> str:
        """Execute a prompt using the Perplexity model.

        Args:
            prompt: The prompt to execute

        Returns:
            The model's response
        """
        response = self.claude_chat_agent.step(prompt)
        return response.msg.content

    @with_exponential_backoff(max_retries=3)
    def execute_prompt(self, prompt: str) -> str:
        """Execute a prompt using the default model.

        Args:
            prompt: The prompt to execute

        Returns:
            The model's response
        """
        response = self.gpt_chat_agent.step(prompt)
        return response.msg.content

    def execute_prompt_batch(
        self, prompts: List[str], max_concurrent: int = 5
    ) -> List[Dict[str, Any]]:
        """Execute a batch of prompts.

        Args:
            prompts: List of prompts to execute
            max_concurrent: Maximum number of concurrent executions

        Returns:
            List of results from the prompts
        """
        results = self._execute_prompts_with_pool(
            prompts, self.execute_prompt, max_concurrent
        )
        return results

    def _execute_prompts_with_pool(
        self, prompts: List[str], execute_prompt_func: Callable[[str], str], max_concurrent: int
    ) -> List[Dict[str, Any]]:
        """Execute a list of prompts concurrently using a thread pool.

        Args:
            prompts: List of prompts to execute
            execute_prompt_func: The function to call for executing a single prompt
            max_concurrent: Maximum number of concurrent executions

        Returns:
            List of results from the prompts
        """
        results = []
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = {executor.submit(execute_prompt_func, prompt): prompt for prompt in prompts}
            for future in as_completed(futures):
                prompt = futures[future]
                try:
                    result = future.result()
                    results.append({"prompt": prompt, "response": result, "success": True})
                except Exception as e:
                    logger.error("Error executing prompt: %s - Error: %s", prompt, str(e))
                    results.append({"prompt": prompt, "response": f"Error: {str(e)}", "success": False})
        return results

    def load_prompts(self, path: str) -> List[str]:
        """Load prompts from a JSON file.

        Args:
            path: Path to the JSON file

        Returns:
            List of prompts
        """
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def save_results(self, results: List[Dict[str, Any]], filename: Optional[str] = None) -> str:
        """Save results to a JSON file.

        Args:
            results: List of results to save
            filename: Optional filename

        Returns:
            Path to the saved results
        """
        timestamp = datetime.now().isoformat()
        filename = filename or f"exploit_results_{timestamp}.json"
        file_path = self.output_dir / filename
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=4)
        return str(file_path)
